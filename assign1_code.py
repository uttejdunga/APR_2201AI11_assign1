# -*- coding: utf-8 -*-
"""Untitled31.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fiVW2T6HMFqRTowQpeUmvUo5p3fUOhQ3
"""

# Commented out IPython magic to ensure Python compatibility.
# 1. Imports + download
import io
import urllib.request
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, confusion_matrix
import matplotlib.pyplot as plt

# %matplotlib inline

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
cols = ["age","sex","cp","trestbps","chol","fbs","restecg","thalach","exang","oldpeak","slope","ca","thal","target"]

print("Downloading from:", url)
raw = urllib.request.urlopen(url, timeout=30).read().decode('utf-8')
df = pd.read_csv(io.StringIO(raw), header=None, names=cols, na_values="?")
print("Rows downloaded:", df.shape[0])

# 2. Quick inspect
df.head()

# show missing counts and target distribution (before converting)
print("Missing per column:\n", df.isna().sum())
print("\nOriginal target value counts:\n", df["target"].value_counts())

# 3. Preprocess: make binary target, drop missing rows, define features
# Convert target: 0 = no disease, 1 = disease (original 1-4 -> 1)
df["target"] = (df["target"] > 0).astype(int)

# Drop rows with missing values (simple and safe for this sized dataset)
df_clean = df.dropna().reset_index(drop=True)

# Define categorical and numeric features
categorical_features = ["cp", "restecg", "slope", "thal", "ca"]
numeric_features = [c for c in df_clean.columns if c not in categorical_features + ["target"]]

print("Rows after dropna:", df_clean.shape[0])
print("Numeric features:", numeric_features)
print("Categorical features:", categorical_features)
print("\nBinary target counts:\n", df_clean["target"].value_counts())

# 4. Build pipeline and train
preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numeric_features),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features)
])

clf = Pipeline([
    ("preproc", preprocessor),
    ("logreg", LogisticRegression(solver="liblinear", max_iter=1000, random_state=42))
])

# Split (stratify to keep class ratios)
X = df_clean.drop("target", axis=1)
y = df_clean["target"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)

clf.fit(X_train, y_train)
print("Model trained.")

# 5. Evaluate
y_pred = clf.predict(X_test)
y_proba = clf.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
roc = roc_auc_score(y_test, y_proba)

print(f"Test accuracy: {acc:.4f}")
print(f"Test ROC AUC: {roc:.4f}\n")
print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# ROC plot
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc:.3f}")
plt.plot([0,1],[0,1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve (Logistic Regression)")
plt.legend()
plt.grid(True)
plt.show()